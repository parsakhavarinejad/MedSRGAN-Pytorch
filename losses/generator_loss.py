
```python
# losses/generator_loss.py
import torch
import torch.nn as nn
import torchvision.models as models
import torch.nn.functional as F

class GeneratorLossFunction(nn.Module):
    """
    Calculates the total loss for the Generator, combining content loss,
    adversarial loss, and adversarial feature loss.
    """
    def __init__(self, device: str = 'cuda', lambda1: float = 1.0, lambda2: float = 0.5,
                 vgg_layers: list[int] = [2, 7, 16, 25, 34],
                 vgg_weights: list[float] = [1.0, 0.5, 0.25, 0.125, 0.0625]) -> None:
        """
        Initializes the GeneratorLossFunction.

        Args:
            device (str): The device (e.g., 'cuda' or 'cpu') where tensors will reside.
            lambda1 (float): Weight for the adversarial loss.
            lambda2 (float): Weight for the adversarial feature loss.
            vgg_layers (list[int]): List of VGG19 layer indices from which to extract features.
            vgg_weights (list[float]): Weights for the MSE loss on VGG features.
        """
        super().__init__()
        self.device = device
        self.lambda1 = lambda1
        self.lambda2 = lambda2

        # Load pre-trained VGG19 model and use its features
        # We only need the feature extraction part up to the max layer specified
        self.vgg = models.vgg19(pretrained=True).features[:max(vgg_layers)+1].to(device).eval()
        self.vgg_layers = vgg_layers
        self.vgg_weights = vgg_weights

        # Freeze VGG parameters to prevent them from being updated during training
        for param in self.vgg.parameters():
            param.requires_grad = False

        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()

    def vgg_extract(self, x: torch.Tensor) -> list[torch.Tensor]:
        """
        Extracts features from specified VGG19 layers.

        Args:
            x (torch.Tensor): Input tensor to the VGG network.

        Returns:
            list[torch.Tensor]: A list of feature maps extracted from the specified VGG layers.
        """
        layers_output = []
        for i, layer in enumerate(self.vgg):
            x = layer(x)
            if i in self.vgg_layers:
                layers_output.append(x)
        return layers_output

    def content_loss(self, hr_image: torch.Tensor, sr_image: torch.Tensor, lambda_l1: float = 0.2) -> torch.Tensor:
        """
        Calculates the content loss, which is a combination of L1 loss and
        weighted MSE loss on VGG features.

        Args:
            hr_image (torch.Tensor): High-resolution ground truth image.
            sr_image (torch.Tensor): Super-resolved image generated by the Generator.
            lambda_l1 (float): Weight for the L1 loss component.

        Returns:
            torch.Tensor: The total content loss.
        """
        # Extract VGG features for both HR and SR images
        hr_features = self.vgg_extract(hr_image)
        sr_features = self.vgg_extract(sr_image)

        # Calculate weighted MSE loss on VGG features
        feature_loss = sum(
            self.vgg_weights[i] * self.mse_loss(sr_features[i], hr_features[i])
            for i in range(len(self.vgg_layers))
        )

        # Calculate L1 loss between HR and SR images
        l1_loss = self.l1_loss(hr_image, sr_image)

        # Combine L1 loss and feature loss
        return lambda_l1 * l1_loss + feature_loss

    def adversarial_loss(self, discriminator: nn.Module, lr_image: torch.Tensor,
                         hr_image: torch.Tensor, sr_image: torch.Tensor) -> torch.Tensor:
        """
        Calculates the adversarial loss for the Generator.
        The Generator tries to fool the Discriminator, so it wants the Discriminator
        to output 1 (real) for its generated (fake) images.

        Args:
            discriminator (nn.Module): The Discriminator network.
            lr_image (torch.Tensor): Low-resolution input image.
            hr_image (torch.Tensor): High-resolution ground truth image (not directly used for G_adv_loss, but for D_real_preds).
            sr_image (torch.Tensor): Super-resolved image generated by the Generator.

        Returns:
            torch.Tensor: The adversarial loss for the Generator.
        """
        # Get discriminator's prediction for the super-resolved image
        # We only need the final sigmoid output, which is the last element in the tuple
        sr_preds = discriminator(lr_image, sr_image)[-1]

        # Generator wants sr_preds to be close to 1 (real)
        # Using -log(D(G(z))) for non-saturating loss
        adv_loss = -torch.log(sr_preds + 1e-8) # Add epsilon for numerical stability
        return adv_loss.mean()

    def adversarial_feature_loss(self, discriminator: nn.Module, lr_image: torch.Tensor,
                                 hr_image: torch.Tensor, sr_image: torch.Tensor) -> torch.Tensor:
        """
        Calculates the adversarial feature loss (feature matching loss).
        This loss minimizes the difference between the intermediate feature maps
        of the Discriminator when fed with real HR images and generated SR images.

        Args:
            discriminator (nn.Module): The Discriminator network.
            lr_image (torch.Tensor): Low-resolution input image.
            hr_image (torch.Tensor): High-resolution ground truth image.
            sr_image (torch.Tensor): Super-resolved image generated by the Generator.

        Returns:
            torch.Tensor: The adversarial feature loss.
        """
        # Get intermediate discriminator features for real HR and generated SR images
        # Exclude the last element which is the sigmoid output
        d_real_features = discriminator(lr_image, hr_image)[:-1]
        d_fake_features = discriminator(lr_image, sr_image)[:-1]

        # Calculate weighted MSE loss on these intermediate features
        # The original code uses self.weights, which should be self.vgg_weights if that's the intention
        # Assuming the weights are meant to apply to these discriminator features as well,
        # and that the number of features matches the number of weights.
        # If not, a simple sum of MSE losses or different weights might be needed.
        adv_feat_loss = sum(
            self.vgg_weights[i] * self.mse_loss(d_real_features[i], d_fake_features[i])
            for i in range(len(d_real_features))
        )
        return adv_feat_loss

    def forward(self, discriminator: nn.Module, lr_image: torch.Tensor,
                hr_image: torch.Tensor, sr_image: torch.Tensor) -> torch.Tensor:
        """
        Calculates the total Generator loss.

        Args:
            discriminator (nn.Module): The Discriminator network.
            lr_image (torch.Tensor): Low-resolution input image.
            hr_image (torch.Tensor): High-resolution ground truth image.
            sr_image (torch.Tensor): Super-resolved image generated by the Generator.

        Returns:
            torch.Tensor: The total Generator loss.
        """
        # Original weighting from the notebook: 0.6 * content_loss + 0.2 * adv_loss + 0.2 * adv_feature_loss
        total_g_loss = (
            0.6 * self.content_loss(hr_image, sr_image) +
            0.2 * self.lambda1 * self.adversarial_loss(discriminator, lr_image, hr_image, sr_image) +
            0.2 * self.lambda2 * self.adversarial_feature_loss(discriminator, lr_image, hr_image, sr_image)
        )
        return total_g_loss
